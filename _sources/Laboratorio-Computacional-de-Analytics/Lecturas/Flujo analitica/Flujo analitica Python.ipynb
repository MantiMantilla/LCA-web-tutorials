{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Archivos/miad4.png\" width=800x>\n",
    "\n",
    "# ¿Cuál es el flujo de trabajo para analítica de datos en Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta ocasión haremos un preámbulo sobre el propósito de los paquetes Numpy y Pandas como columna vertebral del análisis de datos en Python, enfatizando la versatilidad que ofrecen para análisis de datos al tener gran cantidad de métodos útiles incorporados, y de acoplarse fácilmente con paquetes especializados. \n",
    "\n",
    "Hasta el momento, hemos cubierto el uso de Python en su expresión más básica; es decir, considerando únicamente sus estructuras de datos nativas, así como los condicionales, ciclos y funciones. Si bien a partir de esa base es posible lograr casi cualquier desarrollo, una de las ventajas de un lenguaje abierto es que la comunidad desarrolla paquetes de código especializado para suplir necesidades en áreas de aplicación específicas. El análisis de datos, lejos de ser la excepción, es un área en la que continuamente se están desarrollando y actualizando paquetes concebidos para facilitar la labor del analista. Así, la práctica estándar es partir de paquetes que proveen “código pre-cocido” para resolver tareas comunes y no tener que programar todo desarrollo desde cero.  \n",
    "\n",
    "Uno de los paquetes más populares y útiles en Python se denomina Numpy y permite desarrollar operaciones numéricas de forma fácil y eficiente, al introducir la estructura de datos “arreglo”, similar a las listas, pero con soporte para operaciones, desde las más sencillas, como la suma término a término (que no existe en las listas), hasta otras más elaboradas como multiplicaciones de matrices y tensores y análisis numérico, en general (algebra lineal, probabilidad). Numpy, debido a las características descritas, es un paquete ampliamente usado el contexto de computación científica, y también es responsable de mucha de la computación en análisis de datos, ya que, como veremos después, no pocos paquetes especializados se basan en Numpy. \n",
    "\n",
    "Se puede decir que el paquete central para análisis de datos en Python es Pandas, ya que facilita la representación de datos con base en estructuras de datos convenientes, acompañadas de métodos que permiten numerosas operaciones sobre ellos. Estas estructuras de datos básicas son las Series y los DataFrames. Como su nombre lo indica, las Series corresponden a secuencias de datos, similares a las listas, pero permiten indexación diferente al orden (por ejemplo, se pueden utilizar fechas, etiquetas de texto, entre otros, para indexar los datos). Adicionalmente, las series soportan una variedad de operaciones similares a las que se aplicarían a columnas de una hoja de cálculo, como veremos cuando hablemos de DataFrames. \n",
    "\n",
    "La estructura de datos más usada en Analytics es el DataFrame, que puede ser interpretado como una colección de Series. Dicho de otra manera, un DataFrame es análogo a una hoja de cálculo, en la que se tienen columnas (Series) con datos de diferentes tipos, cada una identificada con un nombre o índice, y con una variedad de métodos disponibles que permiten hacer operaciones cotidianas de análisis de datos. Por ejemplo, calcular sumas, promedios, máximos, mínimos, o incluso generar gráficas, entre muchas otras operaciones, se hace tan trivialmente como se haría una hoja de cálculo: invocando el nombre de una función, y en algunos casos, especificando ciertos parámetros de entrada. \n",
    "\n",
    "En este punto, cabría la pregunta: “¿por qué usaría Pandas (teniendo en cuenta que implica programar) si hace lo mismo que una hoja de cálculo?”. \n",
    "\n",
    "Una primera respuesta es que las instrucciones que utilizamos en hojas de cálculo son, en efecto, una forma básica de programación. Precisamente por eso, usar una herramienta como Pandas no debería resultar muy complicado. \n",
    "\n",
    "Sin embargo, sí existen ventajas objetivas de Pandas sobre las hojas de cálculo: la primera es el desempeño computacional superior que ofrece Pandas, permitiendo la manipulación de conjuntos de datos más grandes. La segunda ventaja es la facilidad para pasar de Pandas a otros paquetes especializados de análisis de datos que ofrecen herramientas más allá del alcance de hojas de cálculo (e.g., scikit learn, tensorflow). Una tercera ventaja tiene que ver con la versatilidad de Pandas para importar y exportar datos en una amplia variedad de formatos, permitiendo agregar datos desde JSON, HTML, SQL, XLS, entre muchos otros, de forma simple, lo cual es conveniente puesto que, por lo general, los datos se encuentran en fuentes diversas. \n",
    "\n",
    "El objetivo de este módulo del curso es lograr importar datos desde cualquier formato hasta Pandas como DataFrames o Series, allí explorarlos y manipularlos utilizando los métodos disponibles, así como algunos paquetes adicionales de exploración visual. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hola. \n",
    "El análisis exploratorio es todo: ahí es donde surgen y se perfeccionan las preguntas.\n",
    "\n",
    "Al hacer análisis exploratorio es muy frecuente preguntarse si alguna variable, o algún valor específico que tome una variable, dan lugar a algún comportamiento de interés. En este podcast presentaré algunos ejemplos de estos casos, relacionándolos con las habilidades de programación necesarias para llevar a cabo los análisis. \n",
    "\n",
    "En el contexto científico, es muy común ver estudios clínicos o económicos en los que se pregunta sobre el impacto de algún medicamento o alguna política en diferentes poblaciones. Por ejemplo, se quiere saber si cierto tratamiento tiene un mayor efecto en mujeres que en hombres, o si las personas con cierto nivel educativo perciben mayores ingresos. Al momento de presentar conclusiones definitivas es necesario llevar a cabo pruebas estadísticas formales que confirmen la existencia de los efectos evaluados. Sin embargo, estas preguntas no se formulan a la ligera, sino que provienen de pasos previos de análisis exploratorio. En el contexto de Analytics en las organizaciones, esta exploración de preguntas de interés parte del conocimiento del negocio y el análisis descriptivo de los datos. Las estrategias más ampliamente utilizadas para este propósito son los descriptivos generales de las variables de interés (que hemos cubierto en la semana anterior), los filtros y análisis por grupos (que son tema de este podcast), y el análisis visual (que cubriremos en el próximo podcast). \n",
    " \n",
    "\n",
    "El filtrado de datos es una de las prácticas más comunes en análisis y está directamente relacionado con las estructuras condicionales que cubrimos en la Semana 1. En esencia, queremos observar un sub-conjunto de los datos que cumpla con una o más condiciones. Por ejemplo, en una base de datos del personal de una organización, podríamos preguntarnos por las condiciones de quienes están próximos a pensionarse. En este caso, querríamos concentrarnos en aquellos registros (filas) en los que la variable o columna correspondiente a la edad tome un valor por encima de cierta edad. Si lo pensamos en términos de programación básica en Python, esto implicaría tener que ejecutar un ciclo (for) que recorra todas las filas y en cada caso ejecute un condicional (if) que se pregunte si la edad de la persona actual es mayor o igual a la edad especificada para considerarse próximo a pensionarse. Lo anterior tiene como desventaja que habría que escribir explícitamente el código, lo cual demanda tiempo, esfuerzo, y viene con una alta probabilidad de que el código escrito no sea óptimo en términos de eficiencia computacional. Por esta razón, Pandas dispone de una sintaxis que permite hacer filtrados complejos eficientemente  sin necesidad de programar explícitamente los ciclos y condicionales necesarios. Esta funcionalidad se basa en la indexación lógica de estructuras de datos que estudiamos en los casos de las listas y los arreglos de numpy, en las cuales podíamos usar condiciones de verdadero y falso para seleccionar las posiciones de una estructura de datos que queríamos ver. En el caso de DataFrames y Series en Pandas podemos aplicar los mismos conceptos, como veremos en los tutoriales de esta semana. Podemos imponer condiciones en una o más de las columnas de un conjunto de datos y concentrarnos solo en los casos que nos interesan, valiéndonos de los operadores “or”, “and”, “not” y otros operadores lógicos como “all” o “any” para imponer múltiples condiciones. Por ejemplo, podría interesarnos seleccionar los datos correspondientes a empleados que tengan una edad mayor a cierto valor y menor a cierto valor; o aquellos que estén inscritos en un cierto plan de pensiones u otro; o una combinación de ambas. Al ser capaces de extraer rápidamente aquellos datos que cumplen con ciertos criterios, podemos aplicar los análisis descriptivos básicos que conocemos para responder preguntas exploratorias más interesantes. Por ejemplo: ¿será cierto que el salario promedio de quienes están próximos a pensionarse es mayor que el del resto de empleados? O ¿será cierto que el promedio de ventas de un empleado es menor cuando está próximo a pensionarse? El hecho de filtrar los datos ofrece la posibilidad de hacer análisis comparativos para generar y probar hipótesis que pueden dar lugar a preguntas de negocio de interés para la toma de decisiones. \n",
    " \n",
    "\n",
    "El análisis por grupos es una funcionalidad más avanzada que parte de un principio o motivación similar: ¿existe alguna diferencia en los datos que corresponden a cierto grupo o categoría? El método de pandas en el nos concentraremos apara este tipo de análisis es “groupby”. La diferencia fundamental es que mientras en el filtrado imponíamos una o más condiciones sobre el valor de una variable o columna (ej. seleccionar personas en un rango de edad específico), en el análisis por grupos queremos observar o calcular métricas específicas para todos los segmentos en que una o más variables categóricas dividan los datos. Esto es, si los datos contienen una variable categórica relacionada con la ciudad, lo que queremos ver es cómo varía el comportamiento de los datos según todos los posibles valores que tome la variable ciudad. Para enfatizar la diferencia, pensemos que lo que haríamos con el filtrado sería obtener el sub-conjunto de datos en los cuáles la variable “Ciudad” sea igual a cierta ciudad (  DF[ DF[“Ciudad”] == “Bogotá” ]  ) o incluso a varias ( DF[ DF[“Ciudad”] == “Bogotá” ] or DF[“Ciudad”] == “Lima” ] ), pero en cualquier caso el resultado es un único conjunto de datos que cumplen con la condición impuesta. Por el contrario, al hacer análisis por grupos, como veremos en los tutoriales de esta semana, lo que obtendríamos son múltiples conjuntos de datos: uno por cada valor de la variable ciudad, de forma que todos los datos que corresponden a Lima han sido seleccionados de forma separada a los que corresponden a Bogotá y a los que corresponden a Buenos Aires. La mayor ventaja de esto es que, una vez hemos aplicado el agrupamiento de datos, podemos calcular fácilmente métricas sobre los conjuntos separados, de forma que podemos hacer análisis comparativos más elaborados que los que permite el filtrado básico. Por ejemplo, si aplicamos el método para calcular el promedio (mean) o la desviación (stddev) sobre los datos agrupados tendremos, no un único promedio, sino los promedios independientes de los datos que correspondan a cada ciudad. Con esto, resultará muy sencillo evaluar si, por ejemplo, los precios de los restaurantes en Bogotá son más baratos en promedio, pero tienen mayor desviación que en las demás ciudades. \n",
    " \n",
    "\n",
    "Por supuesto, tanto el filtrado como el agrupamiento pueden implementarse desde cero en Python; incluso, es posible implementar agrupamiento a través de un código que aplique múltiples filtrados. Sin embargo, la sintaxis y métodos que ofrece Pandas facilita bastante la implementación de estos análisis. Esto agrega mucho valor al proceso de Analytics porque dichos análisis típicamente ocurren en una dinámica de ensayo y error, en la que estamos probando diferentes hipótesis antes de llegar a un hallazgo de más trascendencia para abordar problemas de negocio. El primer tutorial de esta semana abordará los temas introducidos en este podcast, mientras que el segundo estará relacionado con el análisis visual, que es el complemento ideal para el análisis exploratorio.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
